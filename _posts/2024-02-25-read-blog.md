---
layout: post
title: Let's make a Teeny Tiny compiler
date:   2024-02-25
categories: 勝手翻訳 Unix
---





## Austin Z. Henley {#austin-z.-henley style="margin-bottom:-2px;"}

I work on software.





<azh321@gmail.com>\
[\@austinzhenley](https://twitter.com/austinzhenley)\
[github/AZHenley](https://github.com/AZHenley)\





------------------------------------------------------------------------


[Home](../index.html) \| [Publications](../publications.html) \|
[Blog](../blog.html){style="text-decoration: underline;"}





------------------------------------------------------------------------

# Let\'s make a Teeny Tiny compiler, part 1 {#lets-make-a-teeny-tiny-compiler-part-1 .blogtitle}

[5/5/2020]{.small}\
\
![](img/20240225_knightdragon.jpeg){.center
style="max-width:60%;border:1px solid black;"}

*This is the first post in a three part series. Check out [part
2](teenytinycompiler2.html) and [part 3](teenytinycompiler3.html) when
you are ready.*

It is a beautiful day outside, so let\'s make a compiler. You don\'t
need any knowledge of how compilers work to follow along. We are going
to use Python to implement our own programming language, Teeny Tiny,
that will compile to C code. It will take about 500 lines of code and
provide the initial infrastructure needed to customize and extend the
compiler into your own billion dollar production-ready compiler.

This tutorial is a series of posts that go step by step in building a
working compiler. All of the source code can be found in the GitHub
[repo](https://github.com/AZHenley/teenytinycompiler). If you follow
along with all the posts, I guesstimate that it will take you only a few
hours.

The Teeny Tiny language we are going to implement is a dialect of
[BASIC](https://en.wikipedia.org/wiki/BASIC). The syntax is clean and
simple. If you prefer a
[C](https://en.wikipedia.org/wiki/C_(programming_language))-like syntax
then it will be trivial to modify the compiler at the end. Here is an
example program in our Teeny Tiny language:

``` prettyprint
PRINT "How many fibonacci numbers do you want?"
INPUT nums

LET a = 0
LET b = 1
WHILE nums > 0 REPEAT
    PRINT a
    LET c = a + b
    LET a = b
    LET b = c
    LET nums = nums - 1
ENDWHILE    
```

This program prints out terms of the fibonacci sequence based on the
user\'s input: 0 1 1 2 3 5 8 13\...

Our language will allow a variety of the basic operations that you\'d
expect from a programming language. In particular, it will support:

-   Numerical variables
-   Basic arithmetic
-   If statements
-   While loops
-   Print text and numbers
-   Input numbers
-   Labels and goto
-   Comments

Although this is a standard subset of features, you may notice that
there are no functions, no arrays, no way to read/write from a file, and
not even an else statement. But with just this small set of constructs,
you can actually do a lot. It will also setup the compiler in such a way
that many other features will be straight forward to add later.

### Compiler Overview

![](img/20240225_compilersteps.png){.center style="max-width:75%;"}

Our compiler will follow a three step process that is illustrated above.
First, given the inputted source code, it will break the code up into
*tokens*. These are like words and punctuation in English. Second, it
will *parse* the tokens to make sure they are in an order that is
allowed in our language. Just like English sentences follow specific
structures of verbs and nouns. Third, it will *emit* the C code that our
language will translate to.

We will use these three steps as the main organization for our code. The
lexer, parser, and emitter will each have their own Python code file.
This tutorial is broken up into 3 parts based on these steps as well. If
you were to extend the compiler, there are some additional steps you
would add, but we will hold off on discussing those.

### Lexer Overview

The first module of our compiler is called the *lexer*. Given a string
of Teeny Tiny code, it will iterate character by character to do two
things: decide where each token starts/stops and what type of token it
is. If the lexer is unable to do this, then it will report an error for
an invalid token.

![](img/20240225_tokens.png){.center style="max-width:50%;"}

The figure demonstrates an example input and output of the lexer. Given
the Teeny Tiny code, the lexer must determine where the tokens are along
with the type (e.g., keyword). You can see that spaces aren\'t
recognized as tokens, but the lexer will use them as one way to know
when a token ends.

Let\'s finally get into some code, starting with the structure of the
lexer in the file **lex.py**:

``` {.prettyprint .lang-python}
class Lexer:
    def __init__(self, source):
        pass

    # Process the next character.
    def nextChar(self):
        pass

    # Return the lookahead character.
    def peek(self):
        pass

    # Invalid token found, print error message and exit.
    def abort(self, message):
        pass
        
    # Skip whitespace except newlines, which we will use to indicate the end of a statement.
    def skipWhitespace(self):
        pass
        
    # Skip comments in the code.
    def skipComment(self):
        pass

    # Return the next token.
    def getToken(self):
        pass
```

I like to sketch out all the functions that I think I will need, then go
back and fill them in. The function **getToken** will be the meat of the
lexer. It will be called each time the compiler is ready for the next
token and it will do the work of classifying tokens. **nextChar** and
**peek** are helper functions for looking at the next character.
**skipWhitespace** consumes the spaces and tabs that we don\'t care
about. **abort** is what we will use to report an invalid token.

The lexer needs to keep track of the current position in the input
string and the character at that position. We will initialize these in
the constructor:

``` {.prettyprint .lang-python}
    def __init__(self, source):
        self.source = source + '\n' # Source code to lex as a string. Append a newline to simplify lexing/parsing the last token/statement.
        self.curChar = ''   # Current character in the string.
        self.curPos = -1    # Current position in the string.
        self.nextChar()
```

The lexer needs the input code and we append a newline to it (this just
simplifies some checks later on). **curChar** is what the lexer will
constantly check to decide what kind of token it is. Why not just do
*source\[curPos\]*? Because that would litter the code with bounds
checking. Instead we do this in **nextChar**:

``` {.prettyprint .lang-python}
    # Process the next character.
    def nextChar(self):
        self.curPos += 1
        if self.curPos >= len(self.source):
            self.curChar = '\0'  # EOF
        else:
            self.curChar = self.source[self.curPos]
```

This increments the lexer\'s current position and updates the current
character. If we reach the end of the input, set the character to the
end-of-file marker. This is the only place we will modify curPos and
curChar. But sometimes we want to look ahead to the next character
without updating curPos:

``` {.prettyprint .lang-python}
    # Return the lookahead character.
    def peek(self):
        if self.curPos + 1 >= len(self.source):
            return '\0'
        return self.source[self.curPos+1]
```

We should make sure these functions work. Let\'s test them by create a
new file **teenytiny.py**:

``` {.prettyprint .lang-python}
from lex import *

def main():
    source = "LET foobar = 123"
    lexer = Lexer(source)

    while lexer.peek() != '\0':
        print(lexer.curChar)
        lexer.nextChar()

main()
```

Run this and the output should be every character of the input string,
*LET foobar = 123*, on a new line:

``` prettyprint
L
E 
T 

f 
o 
o 
b 
a 
r 

= 

1 
2 
3 
```

### Classifying Tokens

But we don\'t just want characters, we want tokens! We need to plan how
combining individual characters together makes a token, which works much
like a state machine. Here are the main lexer rules for the Teeny Tiny
language:

-   Operator. One or two consecutive characters that matches: + - \* / =
    == != \> \< \>= \<=
-   String. Double quotation followed by zero or more characters and a
    double quotation. Such as: \"hello, world!\" and \"\"
-   Number. One or more numeric characters followed by an optional
    decimal point and one or more numeric characters. Such as: 15 and
    3.14
-   Identifier. An alphabetical character followed by zero or more
    alphanumeric characters.
-   Keyword. Exact text match of: LABEL, GOTO, PRINT, INPUT, LET, IF,
    THEN, ENDIF, WHILE, REPEAT, ENDWHILE

Next we will start our **getToken** function in our **Lexer** class:

``` {.prettyprint .lang-python}
    # Return the next token.
    def getToken(self):
        # Check the first character of this token to see if we can decide what it is.
        # If it is a multiple character operator (e.g., !=), number, identifier, or keyword then we will process the rest.
        if self.curChar == '+':
            pass    # Plus token.
        elif self.curChar == '-':
            pass    # Minus token.
        elif self.curChar == '*':
            pass    # Asterisk token.
        elif self.curChar == '/':
            pass    # Slash token.
        elif self.curChar == '\n':
            pass    # Newline token.
        elif self.curChar == '\0':
            pass    # EOF token.
        else:
            # Unknown token!
            pass
            
        self.nextChar()
```

This will detect a few possible tokens, but doesn\'t do anything useful
yet. What we need next is a **Token** class to keep track of what type
of token it is and the exact text from the code. Place this in
**lex.py** for now:

``` {.prettyprint .lang-python}
# Token contains the original text and the type of token.
class Token:   
    def __init__(self, tokenText, tokenKind):
        self.text = tokenText   # The token's actual text. Used for identifiers, strings, and numbers.
        self.kind = tokenKind   # The TokenType that this token is classified as.
```

To specify what type a token is, we will create the **TokenType** class
as an enum. It looks long, but it just specifies every possible token
our language allows. Add *import enum* to the top of **lex.py** and add
this class:

``` {.prettyprint .lang-python}
# TokenType is our enum for all the types of tokens.
class TokenType(enum.Enum):
    EOF = -1
    NEWLINE = 0
    NUMBER = 1
    IDENT = 2
    STRING = 3
    # Keywords.
    LABEL = 101
    GOTO = 102
    PRINT = 103
    INPUT = 104
    LET = 105
    IF = 106
    THEN = 107
    ENDIF = 108
    WHILE = 109
    REPEAT = 110
    ENDWHILE = 111
    # Operators.
    EQ = 201  
    PLUS = 202
    MINUS = 203
    ASTERISK = 204
    SLASH = 205
    EQEQ = 206
    NOTEQ = 207
    LT = 208
    LTEQ = 209
    GT = 210
    GTEQ = 211
```

Now we can expand **getToken** to actually do something when it detects
a specific token:

``` {.prettyprint .lang-python}
    # Return the next token.
    def getToken(self):
        token = None

        # Check the first character of this token to see if we can decide what it is.
        # If it is a multiple character operator (e.g., !=), number, identifier, or keyword then we will process the rest.
        if self.curChar == '+':
            token = Token(self.curChar, TokenType.PLUS)
        elif self.curChar == '-':
            token = Token(self.curChar, TokenType.MINUS)
        elif self.curChar == '*':
            token = Token(self.curChar, TokenType.ASTERISK)
        elif self.curChar == '/':
            token = Token(self.curChar, TokenType.SLASH)
        elif self.curChar == '\n':
            token = Token(self.curChar, TokenType.NEWLINE)
        elif self.curChar == '\0':
            token = Token('', TokenType.EOF)
        else:
            # Unknown token!
            pass
            
        self.nextChar()
        return token
```

This code sets up the lexer to detect the basic arithmetic operators
along with new lines and the end of file marker. The *else* clause is
for capturing everything that won\'t be allowed.

Let\'s change **main** to see whether this works or not so far:

``` {.prettyprint .lang-python}
def main():
    source = "+- */"
    lexer = Lexer(source)

    token = lexer.getToken()
    while token.kind != TokenType.EOF:
        print(token.kind)
        token = lexer.getToken()
```

If you run this, you should see something like:

``` prettyprint
TokenType.PLUS
TokenType.MINUS
Traceback (most recent call last):
  File "e:/projects/teenytiny/part1/teenytiny.py", line 12, in 
    main()
  File "e:/projects/teenytiny/part1/teenytiny.py", line 8, in main
    while token.kind != TokenType.EOF:
AttributeError: 'NoneType' object has no attribute 'kind'
```

Uhoh! Something went wrong. The only way **getToken** returns *None* is
if the *else* branch is taken. We should handle this a little better.
Add *import sys* to the top of **lex.py** and define the **abort**
function like:

``` {.prettyprint .lang-python}
    # Invalid token found, print error message and exit.
    def abort(self, message):
        sys.exit("Lexing error. " + message)
```

And replace the *else* in **getToken** with:

``` {.prettyprint .lang-python}
        else:
            # Unknown token!
            self.abort("Unknown token: " + self.curChar)
```

Now run the program again\...

``` prettyprint
TokenType.PLUS
TokenType.MINUS
Lexing error. Unknown token: 
```

There is still an issue, but now we can make a little more sense of it.
It looks like something went wrong after the first two tokens. The
unknown token is invisible. Looking back at the input string, you may
notice we aren\'t handling whitespace! We need to implement the
**skipWhitespace** function:

``` {.prettyprint .lang-python}
    # Skip whitespace except newlines, which we will use to indicate the end of a statement.
    def skipWhitespace(self):
        while self.curChar == ' ' or self.curChar == '\t' or self.curChar == '\r':
            self.nextChar()
```

Now put *self.skipWhitespace()* as the first line of **getToken**. Run
the program and you should see the output:

``` prettyprint
TokenType.PLUS
TokenType.MINUS
TokenType.ASTERISK
TokenType.SLASH
TokenType.NEWLINE
```

Progress!

At this point, we can move on to lexing the operators that are made up
of two characters, such as *==* and *\>=*. All of these operators will
be lexed in the same fashion: check the first character, then peek at
the second character to see what it is before deciding what to do. Add
this after the *elif* for the *SLASH* token in **getToken**:

``` {.prettyprint .lang-python}
        elif self.curChar == '=':
            # Check whether this token is = or ==
            if self.peek() == '=':
                lastChar = self.curChar
                self.nextChar()
                token = Token(lastChar + self.curChar, TokenType.EQEQ)
            else:
                token = Token(self.curChar, TokenType.EQ)
```

Using the **peek** function allows us to look at what the next character
will be without discarding the *curChar*. Here is the code for the
remaining operators which work the same way:

``` {.prettyprint .lang-python}
        elif self.curChar == '>':
            # Check whether this is token is > or >=
            if self.peek() == '=':
                lastChar = self.curChar
                self.nextChar()
                token = Token(lastChar + self.curChar, TokenType.GTEQ)
            else:
                token = Token(self.curChar, TokenType.GT)
        elif self.curChar == '<':
                # Check whether this is token is < or <=
                if self.peek() == '=':
                    lastChar = self.curChar
                    self.nextChar()
                    token = Token(lastChar + self.curChar, TokenType.LTEQ)
                else:
                    token = Token(self.curChar, TokenType.LT)
        elif self.curChar == '!':
            if self.peek() == '=':
                lastChar = self.curChar
                self.nextChar()
                token = Token(lastChar + self.curChar, TokenType.NOTEQ)
            else:
                self.abort("Expected !=, got !" + self.peek())
```

The only operator that is a bit different is *!=*. That is because the
*!* character is not valid on its own, so it must be followed by *=*.
The other characters are valid on their own, but the lexer is greedy and
will accept it as one of the multi-character operators if possible.

We can test these operators by updating the input to *\"+- \*/ \>\>= =
!=\"* which should give you the following output when you run the
program:

``` prettyprint
TokenType.PLUS
TokenType.MINUS
TokenType.ASTERISK
TokenType.SLASH
TokenType.GT
TokenType.GTEQ
TokenType.EQ
TokenType.NOTEQ
TokenType.NEWLINE
```

The program now accepts all of the language\'s operators. So what is
left? We need to add support for comments, strings, numbers,
identifiers, and keywords. Let\'s work through these one by one and test
as we go.

The *\#* character will indicate the start of a comment. Whenever the
lexer sees it, we know to ignore all the text after it until a newline.
Comments are not tokens, but the lexer will discard all this text so
that it can find the next thing we care about. It is also important that
we don\'t throw away the newline at the end of the comment since that is
its own token and may still be needed. Fill in **skipComment**:

``` {.prettyprint .lang-python}
    # Skip comments in the code.
    def skipComment(self):
        if self.curChar == '#':
            while self.curChar != '\n':
                self.nextChar()
```

Easy enough! Now call it from **nextToken**, such that the first few
lines of the function look like:

``` {.prettyprint .lang-python}
    # Return the next token.
    def getToken(self):
        self.skipWhitespace()
        self.skipComment()
        token = None
    ...
```

Test it out with the input *\"+- \# This is a comment!\\n \*/\"* and you
should see:

``` prettyprint
TokenType.PLUS
TokenType.MINUS
TokenType.NEWLINE
TokenType.ASTERISK
TokenType.SLASH
TokenType.NEWLINE
```

Notice that the comment is completely ignored!

Our language supports printing a string, which starts with a double
quotation mark and continues until another quotation mark. We won\'t
allow some special characters to make it easier to compile to C later
on. Add the following code to **getToken**\'s big block of else if
statements:

``` {.prettyprint .lang-python}
        elif self.curChar == '\"':
            # Get characters between quotations.
            self.nextChar()
            startPos = self.curPos

            while self.curChar != '\"':
                # Don't allow special characters in the string. No escape characters, newlines, tabs, or %.
                # We will be using C's printf on this string.
                if self.curChar == '\r' or self.curChar == '\n' or self.curChar == '\t' or self.curChar == '\\' or self.curChar == '%':
                    self.abort("Illegal character in string.")
                self.nextChar()

            tokText = self.source[startPos : self.curPos] # Get the substring.
            token = Token(tokText, TokenType.STRING)
```

You\'ll see the code is just a while loop that continues until the
second quotation mark. It\'ll abort with an error message if any of the
invalid characters are found. Something different from the other tokens
we have covered so far: we set the token\'s text to the content of the
string (minus the quotation marks).

Update the input again with *\"+- \\\"This is a string\\\" \# This is a
comment!\\n \*/\"* and run the program:

``` prettyprint
TokenType.PLUS
TokenType.MINUS
TokenType.STRING
TokenType.NEWLINE
TokenType.ASTERISK
TokenType.SLASH
TokenType.NEWLINE
```

Moving right along to numbers. Our language defines a number as one or
more digits (0-9) followed by an optional decimal point that must be
followed by at least one digit. So 48 and 3.14 are allowed but .9 and 1.
are not allowed. We will use the **peek** function again to look ahead
one character. Similar to the string token, we keep track of the start
and end points of the numbers so that we can set the token\'s text to
the actual number.

``` {.prettyprint .lang-python}
        elif self.curChar.isdigit():
            # Leading character is a digit, so this must be a number.
            # Get all consecutive digits and decimal if there is one.
            startPos = self.curPos
            while self.peek().isdigit():
                self.nextChar()
            if self.peek() == '.': # Decimal!
                self.nextChar()

                # Must have at least one digit after decimal.
                if not self.peek().isdigit(): 
                    # Error!
                    self.abort("Illegal character in number.")
                while self.peek().isdigit():
                    self.nextChar()

            tokText = self.source[startPos : self.curPos + 1] # Get the substring.
            token = Token(tokText, TokenType.NUMBER)
```

Test it out with the input *\"+-123 9.8654\*/\"* and you should see:

``` prettyprint
TokenType.PLUS
TokenType.MINUS
TokenType.NUMBER
TokenType.NUMBER
TokenType.ASTERISK
TokenType.SLASH
TokenType.NEWLINE
```

Great, we are almost done with the lexer!

The last big thing is to handle identifiers and keywords. The rules for
an identifier is anything that starts with a alphabetic characters
followed by zero or more alphanumeric characters. But before we call it
a *TokenType.IDENT*, we have to make sure it isn\'t one of our keywords.
Add this to **getToken**:

``` {.prettyprint .lang-python}
        elif self.curChar.isalpha():
            # Leading character is a letter, so this must be an identifier or a keyword.
            # Get all consecutive alpha numeric characters.
            startPos = self.curPos
            while self.peek().isalnum():
                self.nextChar()

            # Check if the token is in the list of keywords.
            tokText = self.source[startPos : self.curPos + 1] # Get the substring.
            keyword = Token.checkIfKeyword(tokText)
            if keyword == None: # Identifier
                token = Token(tokText, TokenType.IDENT)
            else:   # Keyword
                token = Token(tokText, keyword)
```

Fairly similar to the other tokens. But we need to define
**checkIfKeyword** in the **Token** class:

``` {.prettyprint .lang-python}
    @staticmethod
    def checkIfKeyword(tokenText):
        for kind in TokenType:
            # Relies on all keyword enum values being 1XX.
            if kind.name == tokenText and kind.value >= 100 and kind.value < 200:
                return kind
        return None
```

This just checks whether the token is in the list of keywords, which we
have arbitrarily set to having 101-199 as their enum values.

Alright, test identifiers and keywords with the input string *\"IF+-123
foo\*THEN/\"*

``` prettyprint
TokenType.IF
TokenType.PLUS
TokenType.MINUS
TokenType.NUMBER
TokenType.IDENT
TokenType.ASTERISK
TokenType.THEN
TokenType.SLASH
TokenType.NEWLINE
```

And what the output looks like from the terminal:

![](img/20240225_teenytinyoutput1.png){.center style="max-width:75%;"}

There we have it. Our lexer can correctly identify every token that our
language needs! We have successfully completed the first module of our
compiler.

If you think this is underwhelming, don\'t give up yet! I think the
lexer is actually the most tedious yet least interesting part of
compilers. Next up we will *parse* the code, that is make sure the
tokens are in an order that makes sense, and then we will *emit* code.

The source code so far can be found in its entirety in the Github
[repo](https://github.com/AZHenley/teenytinycompiler).

------------------------------------------------------------------------

Continue on to [part 2](teenytinycompiler2.html) of this tutorial. Other
recommended reading:

-   Lexical analysis
    ([Wikipedia](https://en.wikipedia.org/wiki/Lexical_analysis))
-   Writing an Interpreter in Go ([Amazon](https://amzn.to/2Saf28j))
-   Crafting Interpreters ([Amazon](https://amzn.to/3l8FePX),
    [web](https://craftinginterpreters.com/contents.html))

\
\
[There are Amazon affiliate links on this page.]{.small}\
\
\
\




## Austin Z. Henley {#austin-z.-henley style="margin-bottom:-2px;"}

I work on software.





<azh321@gmail.com>\
[\@austinzhenley](https://twitter.com/austinzhenley)\
[github/AZHenley](https://github.com/AZHenley)\





------------------------------------------------------------------------


[Home](../index.html) \| [Publications](../publications.html) \|
[Blog](../blog.html){style="text-decoration: underline;"}





------------------------------------------------------------------------

# Let\'s make a Teeny Tiny compiler, part 2 {#lets-make-a-teeny-tiny-compiler-part-2 .blogtitle}

[6/5/2020]{.small}\
\
![](img/20240225_knightdragon2.jpeg){.center
style="max-width:60%;border:1px solid black;"}

It is a rainy day outside, so let\'s continue working on our Teeny Tiny
compiler. Go read [part 1](teenytinycompiler1.html) if you haven\'t
already, and don\'t forget that the source code can be found in the
GitHub [repo](https://github.com/AZHenley/teenytinycompiler). This part
of the tutorial does need a bit of upfront explanation, but I hope you
stick with it because we are well on our way to completing our compiler!

To recap, we are making a compiler for our own programming language,
Teeny Tiny. The compiler will work in three stages: (1) *lexing*, which
breaks the input code up into small pieces called tokens, (2) *parsing*,
which verifies that the tokens are in an order that our language allows,
and (3) *emitting*, which produces the appropriate C code. We finished
the lexer in [part 1](teenytinycompiler1.html), so now we will be
focusing on the parser.

### Parser Overview

The parser is the component that will make sure the code follows the
correct syntax. It does this by looking at the tokens, one at a time,
and deciding if the ordering is legal as defined by our language.

![](img/20240225_teenyparser.png){.center style="max-width:85%;"}

The figure above is a slightly simplified example of what the parser
will do. The input to the parser is the sequence of tokens and the
output is the parse tree. A parse tree is a more structured
representation of the code than just a text string or a sequence of
tokens. The process to create this tree will be discussed in the
remainder of this post. I know trees can sometimes be scary, but we
won\'t be building any complicated data structure for this. Rather, we
will utilize the *call stack* of our parser to implicitly build the
parse tree as we go.

Before we can verify the code\'s structure or build the parse tree
though, we need to know what structures Teeny Tiny allows\...

### Grammar Overview

Teeny Tiny needs a grammar. This is a formal way to describe all of the
code that is legal in Teeny Tiny. If you\'ve ever had a compiler yell at
you about a syntax error, it is because your code did not follow the
language\'s grammar.

This isn\'t too different than when you learned about English. Take a
look at this example of an English sentence and the corresponding parse
tree:

![](img/20240225_englishparsetree.png){.center style="max-width:75%;"}

The parser will perform a similar process to Teeny Tiny code.

Alright, so how do we come up with the grammar for a language? This is
partially creative and partially technical. A lot of syntax is more of a
stylistic choice than anything. You can even look up the formal grammar
for most programming languages. Grammars are usually written in a
standard notation, but the specifics of the notation don\'t really
matter right now. We just want to be able to implement our parser.

Here is a piece of Teeny Tiny\'s grammar:

``` prettyprint
program ::= {statement}
```

This reads as: there is a grammar rule named program that is made up of
zero or more statement. What is a statement? It is another grammar rule:

``` prettyprint
statement ::= "PRINT" (expression | string) nl
```

The *statement* rule here is defined as the PRINT keyword followed by
either an expression or a string and then a newline. *expression* and
*nl* are other grammar rules that we are referencing. *string* is a type
of token from the lexer.

We can expand the *statement* rule to have multiple options, like so:

``` prettyprint
statement ::= "PRINT" (expression | string) nl
    | "LET" ident "=" expression nl
```

Now the *statement* rule here is defined as one of two options: either a
PRINT statement or a LET statement (the pipe symbol, \|, means *or*). A
LET statement is for assigning a value to a variable. It is defined as
the LET keyword followed by an ident and a \"=\" then an expression and
a newline. *ident* is a type of token from the lexer, which is a
variable identifier. *expression* is another grammar rule which is for
math expressions.

The rules tend to start very general and get more specific. Just like
the English parse tree example from before. Let\'s examine one more type
of statement\...

``` prettyprint
statement ::= "PRINT" (expression | string) nl
    | "LET" ident "=" expression nl
    | "IF" comparison "THEN" nl {statement} "ENDIF" nl
```

Now our language also allows for *IF* statements. The important thing to
notice with this rule is that it is recursive. The *statement* rule
actually references the *statement* rule. This is the power of
programming languages!

Here is the entire grammar for our Teeny Tiny programming language:

``` prettyprint
program ::= {statement}
statement ::= "PRINT" (expression | string) nl
    | "IF" comparison "THEN" nl {statement} "ENDIF" nl
    | "WHILE" comparison "REPEAT" nl {statement} "ENDWHILE" nl
    | "LABEL" ident nl
    | "GOTO" ident nl
    | "LET" ident "=" expression nl
    | "INPUT" ident nl
comparison ::= expression (("==" | "!=" | ">" | ">=" | "<" | "<=") expression)+
expression ::= term {( "-" | "+" ) term}
term ::= unary {( "/" | "*" ) unary}
unary ::= ["+" | "-"] primary
primary ::= number | ident
nl ::= '\n'+
```

You don\'t need to understand all of it just yet, we will be working
through it piece by piece. The magic is that when implementing the
parser, each of these grammar rules will get their own function in the
Python code. It is a one-to-one mapping from grammar to code.

If you\'d like to know a bit more about this notation\... *{}* means
zero or more, *\[\]* means zero or one, *+* means one or more of
whatever is to the left, *()* is just for grouping, and *\|* is a
*logical or*. Words are either references to other grammar rules or to
tokens that we have already defined in our lexer. I denote keywords and
operators as quoted strings of text.

Given this grammar, our parser will produce trees like:

![](img/20240225_teenyparsetree.png){.center style="max-width:90%;"}

As you might imagine, even small programs will generate very large parse
trees. That is ok! We will make the parser do the hard work! (Note: In
the above example I omitted the newline rule from the trees to make it a
bit smaller.)

Ok, let\'s finally get into the code!

### Code Setup

We need to setup the **main** function to use our soon to exist parser.
Update **teenytiny.py** with:

``` {.prettyprint .lang-python}
from lex import *
from parse import *
import sys

def main():
    print("Teeny Tiny Compiler")

    if len(sys.argv) != 2:
        sys.exit("Error: Compiler needs source file as argument.")
    with open(sys.argv[1], 'r') as inputFile:
        source = inputFile.read()

    # Initialize the lexer and parser.
    lexer = Lexer(source)
    parser = Parser(lexer)

    parser.program() # Start the parser.
    print("Parsing completed.")

main()
```

The compiler will now expect a filename as a command line argument to
open for input. The parser object will control the lexer and request a
new token as needed. So next, we have to implement the parser object.
Create **parse.py** and start with:

``` {.prettyprint .lang-python}
import sys
from lex import *

# Parser object keeps track of current token and checks if the code matches the grammar.
class Parser:
    def __init__(self, lexer):
        pass

    # Return true if the current token matches.
    def checkToken(self, kind):
        pass

    # Return true if the next token matches.
    def checkPeek(self, kind):
        pass

    # Try to match current token. If not, error. Advances the current token.
    def match(self, kind):
        pass

    # Advances the current token.
    def nextToken(self):
        pass

    def abort(self, message):
        sys.exit("Error. " + message)
```

Like I mentioned in the previous post, I like to sketch out all of the
methods that I think I need then fill them in.

The **checkToken** and **checkPeek** functions will let the parser
decide which grammar rule to apply next given the current token or the
next one. Add these functions to the **Parser** class:

``` {.prettyprint .lang-python}
    # Return true if the current token matches.
    def checkToken(self, kind):
        return kind == self.curToken.kind

    # Return true if the next token matches.
    def checkPeek(self, kind):
        return kind == self.peekToken.kind
```

In the cases that the parser already knows which grammar rule to apply,
we will use the **match** function. It expects the current token to be
something specific or else it will produce an error. In other cases,
such as when **checkToken** is used, we just want to skip to the next
token with **nextToken**.

``` {.prettyprint .lang-python}
    # Try to match current token. If not, error. Advances the current token.
    def match(self, kind):
        if not self.checkToken(kind):
            self.abort("Expected " + kind.name + ", got " + self.curToken.kind.name)
        self.nextToken()

    # Advances the current token.
    def nextToken(self):
        self.curToken = self.peekToken
        self.peekToken = self.lexer.getToken()
        # No need to worry about passing the EOF, lexer handles that.
```

The parser also needs to be initialized. Here is the code for
**\_\_init\_\_**:

``` {.prettyprint .lang-python}
    def __init__(self, lexer):
        self.lexer = lexer

        self.curToken = None
        self.peekToken = None
        self.nextToken()
        self.nextToken()    # Call this twice to initialize current and peek.
```

We will test everything momentarily.

Now that we have the core mechanics of the parser done, we now need to
actually parse our language. We do this by mapping our grammar to code.
For each rule in our grammar, we have a matching function in the parser.
Remember how we wanted to turn our code into a tree structure? The call
graph of the parsing functions that we will implement next will do that.

### Parsing Statements

We are going to go through our grammar and implement a function for each
rule, one by one. When the rule references another rule, we call that
function. When the rule expects a specific token, we call **match** and
when there are multiple options, we call **checkToken**. We will start
by parsing statements, like PRINT, before moving on to parsing math
expressions.

First, let\'s implement **program**. This function kicks off the parser
and is the parent rule in our grammar. So let\'s refer back to the first
line of our grammar:

``` prettyprint
program ::= {statement}
```

This line just means that the program is made up of 0 or more
statements. To map this to code, add the following code to the end of
the **parser** class:

``` {.prettyprint .lang-python}
    # Production rules.

    # program ::= {statement}
    def program(self):
        print("PROGRAM")

        # Parse all the statements in the program.
        while not self.checkToken(TokenType.EOF):
            self.statement()
```

Just like our grammar says: this will continue to call **statement**
until there is nothing left. See how easy that was to translate? To make
it easier to know what the parser is doing, we will be printing from
each function.

The next rule in our grammar is **statement**, which actually allows for
7 different types of rules. It is the biggest function in our parser,
but it is straight forward if we break it up piece by piece. Inside of
the function we will have an *if* condition for each of the 7 different
statements. Here is the grammar for the first type of statement:

``` prettyprint
statement ::= "PRINT" (expression | string) nl
```

This statement expects a \"PRINT\" token first. Then one of two things
can follow: either a math expression or a string token. Lastly, it
expects a newline. Add this code to the **parser** class:

``` {.prettyprint .lang-python}
    # One of the following statements...
    def statement(self):
        # Check the first token to see what kind of statement this is.

        # "PRINT" (expression | string)
        if self.checkToken(TokenType.PRINT):
            print("STATEMENT-PRINT")
            self.nextToken()

            if self.checkToken(TokenType.STRING):
                # Simple string.
                self.nextToken()
            else:
                # Expect an expression.
                self.expression()

        # Newline.
        self.nl()
```

To make it easier to test, we can go ahead and implement the **nl**
function that handles newlines. We will call this at the end of the
**statement** function, since it applies to all statements. It works by
expecting at least one newline character, but allows for more. Add this
function to the **parser** class:

``` {.prettyprint .lang-python}
    # nl ::= '\n'+
    def nl(self):
        print("NEWLINE")
        
        # Require at least one newline.
        self.match(TokenType.NEWLINE)
        # But we will allow extra newlines too, of course.
        while self.checkToken(TokenType.NEWLINE):
            self.nextToken()
```

Time to test! Create a file, **hello.teeny**, to use as input to the
compiler with *PRINT \"hello, world!\"* as the contents. Run
**teenytiny.py** with the input file as an argument and you should get:

![](img/20240225_teenytinyoutput2.png){.center style="max-width:75%;"}

It works! Our one line of Teeny Tiny code is parsed successfully. Let\'s
try with multiple PRINT statements. Update the input code to:

``` prettyprint
PRINT "hello, world!"
PRINT "second line"
PRINT "and a third..."
```

Rerun the program. You should see:

``` prettyprint
Teeny Tiny Compiler
PROGRAM
STATEMENT-PRINT
NEWLINE
STATEMENT-PRINT
NEWLINE
STATEMENT-PRINT
NEWLINE
Parsing completed.
```

Fantastic! Moving on to the next type of statement, the grammar is as
follows:

``` prettyprint
    | "IF" comparison "THEN" nl {statement} "ENDIF" nl
```

This line starts with an *or*, so it is a continuation from the previous
line in the grammar, and then expects an \"IF\" token. Next it refers to
another grammar rule, **comparison**, which we will define as a function
later. It will allow for the condition, like *foo \> 5*. After that the
language expects a \"THEN\" token followed by a newline. Then comes the
body of the *if* statement, which allows for 0 or more statements.
Lastly, it ends with an \"ENDIF\" token and a newline. Append this
*elif* to the *if* in the **statement** function:

``` {.prettyprint .lang-python}
        # "IF" comparison "THEN" {statement} "ENDIF"
        elif self.checkToken(TokenType.IF):
            print("STATEMENT-IF")
            self.nextToken()
            self.comparison()

            self.match(TokenType.THEN)
            self.nl()

            # Zero or more statements in the body.
            while not self.checkToken(TokenType.ENDIF):
                self.statement()

            self.match(TokenType.ENDIF)
```

Make sure *self.nl()* is still at the end of the **statement** function.
We can\'t test this just yet, so we will move on to the next grammar
rule, which is for the *while* loop. The grammar looks almost identical
to the *if* statement that we just did.

``` prettyprint
    | "WHILE" comparison "REPEAT" nl {statement nl} "ENDWHILE" nl
```

And the code looks similar too. Remember, this is still part of the
**statement** function that we are adding to:

``` {.prettyprint .lang-python}
        # "WHILE" comparison "REPEAT" {statement} "ENDWHILE"
        elif self.checkToken(TokenType.WHILE):
            print("STATEMENT-WHILE")
            self.nextToken()
            self.comparison()

            self.match(TokenType.REPEAT)
            self.nl()

            # Zero or more statements in the loop body.
            while not self.checkToken(TokenType.ENDWHILE):
                self.statement()

            self.match(TokenType.ENDWHILE)
```

You probably see the pattern at this point. The remaining forms of
*statement* are also almost identical to each other:

``` prettyprint
    | "LABEL" ident nl
    | "GOTO" ident nl
    | "LET" ident "=" expression nl
    | "INPUT" ident nl
```

This is the last bit of code for the **statement** function:

``` {.prettyprint .lang-python}
        # "LABEL" ident
        elif self.checkToken(TokenType.LABEL):
            print("STATEMENT-LABEL")
            self.nextToken()
            self.match(TokenType.IDENT)

        # "GOTO" ident
        elif self.checkToken(TokenType.GOTO):
            print("STATEMENT-GOTO")
            self.nextToken()
            self.match(TokenType.IDENT)

        # "LET" ident "=" expression
        elif self.checkToken(TokenType.LET):
            print("STATEMENT-LET")
            self.nextToken()
            self.match(TokenType.IDENT)
            self.match(TokenType.EQ)
            self.expression()

        # "INPUT" ident
        elif self.checkToken(TokenType.INPUT):
            print("STATEMENT-INPUT")
            self.nextToken()
            self.match(TokenType.IDENT)

        # This is not a valid statement. Error!
        else:
            self.abort("Invalid statement at " + self.curToken.text + " (" + self.curToken.kind.name + ")")

        # Newline.
        self.nl()
```

Notice the *else* at the end. If the parser is expecting a statement,
but doesn\'t match any of the 7 types that we defined, it should throw
an error.

Time for some testing. Update the input file to:

``` prettyprint
LABEL loop
PRINT "hello, world!"
GOTO loop
```

Running this should output:

``` prettyprint
Teeny Tiny Compiler
PROGRAM
STATEMENT-LABEL
NEWLINE
STATEMENT-PRINT
NEWLINE
STATEMENT-GOTO
NEWLINE
Parsing completed.
```

We should also try breaking the parser. Try a nonsensical input that
won\'t be parsed by any of the statement forms that we implemented, like
*\"JUMP GOTO\"*, which should show:

``` prettyprint
Teeny Tiny Compiler
PROGRAM
Error! Invalid statement at JUMP (IDENT)
```

One quick thing before we move on. The **program** function currently
can\'t handle newlines at the start of the input, but it is easy to fix
that:

``` {.prettyprint .lang-python}
    # program ::= {statement}
    def program(self):
        print("PROGRAM")

        # Since some newlines are required in our grammar, need to skip the excess.
        while self.checkToken(TokenType.NEWLINE):
            self.nextToken()

        # Parse all the statements in the program.
        while not self.checkToken(TokenType.EOF):
            self.statement()
```

### Parsing Expressions

The parser is really coming together! We have implemented about half of
the language now. We are missing *expressions*. An expression is
something that can be evaluated to a value, like a math expression,
*1+5\*3*, or a boolean expression, *foo\>=10*. Here is the grammar
relevant to expressions:

``` prettyprint
comparison ::= expression (("==" | "!=" | ">" | ">=" | "<" | "<=") expression)+
expression ::= term {( "-" | "+" ) term}
term ::= unary {( "/" | "*" ) unary}
unary ::= ["+" | "-"] primary
primary ::= number | ident
```

This portion of the grammar may look strange or overly complicated to a
reader. Why split it up into these 5 rules? Why not do something
like\... *expression ::= primary {operator primary}*? The answer is
*precedence*.

![](img/20240225_expressiontree.png){.center style="max-width:85%;"}

To achieve different levels of precedence, we organize the grammar rules
sequentially. Operators with higher precedence need to be \"lower\" in
the grammar, such that they are lower in the parse tree. The operators
closest to the tokens in the parse tree (i.e., closest to the leaves of
the tree) will have the highest precedence. Another way to think about
it is how tightly the operators bind to the operands. When parsing, if
there is not an operator at a given level, then it passes through to the
next level and creates a node in the tree with only one child.

By doing this, it enforces the order of operations that you might expect
from a math expression. *1+2\*3* should evaluate to 7, not 9. Looking at
our grammar rules, the unary + and - operators are \"lower\" in our
grammar, so they will have higher precedence than \* and /, which have
higher precedence than the binary + and - operators. The parse trees
above illustrate this. The multiplication operator will always be lower
in the tree than the plus operator. The unary negation operator will be
even lower. If there are more operators with the same precedence, then
they will be processed left to right. More precedence levels (and
operators) can be added by following this pattern.

The *comparison* rule is a bit different. We don\'t want comparison
operators (e.g., !=) to be allowed in math expressions. So to control
where they are allowed (i.e., IF statements and WHILE loops), we have a
special rule for them that requires at least one comparison operator. On
the left and right hand side of the comparison operator is an
*expression*. Now any place we allow only math expressions, we expect
*expression*, and any place we allow boolean expressions, we expect
*comparison*. The code for **comparison** is:

``` {.prettyprint .lang-python}
    # comparison ::= expression (("==" | "!=" | ">" | ">=" | "<" | "<=") expression)+
    def comparison(self):
        print("COMPARISON")

        self.expression()
        # Must be at least one comparison operator and another expression.
        if self.isComparisonOperator():
            self.nextToken()
            self.expression()
        else:
            self.abort("Expected comparison operator at: " + self.curToken.text)

        # Can have 0 or more comparison operator and expressions.
        while self.isComparisonOperator():
            self.nextToken()
            self.expression()
```

To make the code easier to read, I created the **isComparisonOperator**:

``` {.prettyprint .lang-python}
    # Return true if the current token is a comparison operator.
    def isComparisonOperator(self):
        return self.checkToken(TokenType.GT) or self.checkToken(TokenType.GTEQ) or self.checkToken(TokenType.LT) or self.checkToken(TokenType.LTEQ) or self.checkToken(TokenType.EQEQ) or self.checkToken(TokenType.NOTEQ)
```

The code for expression:

``` {.prettyprint .lang-python}
    # expression ::= term {( "-" | "+" ) term}
    def expression(self):
        print("EXPRESSION")

        self.term()
        # Can have 0 or more +/- and expressions.
        while self.checkToken(TokenType.PLUS) or self.checkToken(TokenType.MINUS):
            self.nextToken()
            self.term()
```

The code for term and unary:

``` {.prettyprint .lang-python}
    # term ::= unary {( "/" | "*" ) unary}
    def term(self):
        print("TERM")

        self.unary()
        # Can have 0 or more *// and expressions.
        while self.checkToken(TokenType.ASTERISK) or self.checkToken(TokenType.SLASH):
            self.nextToken()
            self.unary()


    # unary ::= ["+" | "-"] primary
    def unary(self):
        print("UNARY")

        # Optional unary +/-
        if self.checkToken(TokenType.PLUS) or self.checkToken(TokenType.MINUS):
            self.nextToken()        
        self.primary()
```

Finally, the last piece of our grammar. A primary is either a number
token or an ident token, which is a variable name. The code:

``` {.prettyprint .lang-python}
    # primary ::= number | ident
    def primary(self):
        print("PRIMARY (" + self.curToken.text + ")")

        if self.checkToken(TokenType.NUMBER): 
            self.nextToken()
        elif self.checkToken(TokenType.IDENT):
            self.nextToken()
        else:
            # Error!
            self.abort("Unexpected token at " + self.curToken.text)
```

Ready to test expressions??? Put *LET foo = bar \* 3 + 2* in your input
file and run:

``` prettyprint
Teeny Tiny Compiler
PROGRAM
STATEMENT-LET
EXPRESSION
TERM
UNARY
PRIMARY (bar)
UNARY
PRIMARY (3)
TERM
UNARY
PRIMARY (2)
NEWLINE
Parsing completed.
```

Now try:

``` prettyprint
LET foo = bar * 3 + 2
IF foo > 0 THEN
    PRINT "yes!"
ENDIF
```

Which should have the quite long output of:

``` prettyprint
Teeny Tiny Compiler
PROGRAM
STATEMENT-LET
EXPRESSION
TERM
UNARY
PRIMARY (bar)
UNARY
PRIMARY (3)
TERM
UNARY
PRIMARY (2)
NEWLINE
STATEMENT-IF
COMPARISON
EXPRESSION
TERM
UNARY
PRIMARY (foo)
EXPRESSION
TERM
UNARY
PRIMARY (0)
NEWLINE
STATEMENT-PRINT
NEWLINE
NEWLINE
Parsing completed.
```

The parser allows nested loops and if statements too. Try it out.

``` prettyprint
LET foo = bar * 3 + 2
IF foo > 0 THEN
    IF 10 * 10 < 100 THEN
        PRINT bar
    ENDIF
ENDIF
```

Yay! Everything parses!

### Checking Validity

Let\'s do more tests before we go for a victory lap. Run this Teeny Tiny
code:

``` prettyprint
PRINT index
GOTO main
```

So even though our input code conforms to the grammar, it is
nonsensical. We are attempting to PRINT an undeclared variable and GOTO
an undeclared label. The compiler should do something about this! As we
are parsing, the compiler can keep track of which variables and labels
have been declared as well as which labels have been goto\'ed. If an
undeclared variable is referenced, it can print an error. At the end of
parsing, it can also check to make sure that all labels have been
declared. Since labels can be goto\'ed before they are referenced, we
will keep track of both labels and gotos.

Update **\_\_init\_\_** to initialize three sets:

``` {.prettyprint .lang-python}
    def __init__(self, lexer):
        self.lexer = lexer

        self.symbols = set()    # Variables declared so far.
        self.labelsDeclared = set() # Labels declared so far.
        self.labelsGotoed = set() # Labels goto'ed so far.

        self.curToken = None
        self.peekToken = None
        self.nextToken()
        self.nextToken()    # Call this twice to initialize current and peek.
```

Now when we parse a *label* or *goto* statement, we should update the
sets. Replace the code for label and goto in the **statement** function
with:

``` {.prettyprint .lang-python}
        # "LABEL" ident
        elif self.checkToken(TokenType.LABEL):
            print("STATEMENT-LABEL")
            self.nextToken()

            # Make sure this label doesn't already exist.
            if self.curToken.text in self.labelsDeclared:
                self.abort("Label already exists: " + self.curToken.text)
            self.labelsDeclared.add(self.curToken.text)

            self.match(TokenType.IDENT)

        # "GOTO" ident
        elif self.checkToken(TokenType.GOTO):
            print("STATEMENT-GOTO")
            self.nextToken()
            self.labelsGotoed.add(self.curToken.text)
            self.match(TokenType.IDENT)
```

The way this works is that if a label already exists in
*labelsDeclared*, then it means the code is attempting to declare it
twice. That is not allowed, so the compiler aborts with an error
message. If it doesn\'t already exist, then add it to the set. Whenever
GOTO is parsed, insert the label into *labelsGotoed*. This can occur
multiple times.

To verify that there is not a GOTO to an undeclared label, we need to
update **program**. Once the parsing is complete, the compiler just
needs to ensure that all of the labels in *labelsGotoed* are also in
*labelsDeclared*.

``` {.prettyprint .lang-python}
    # program ::= {statement}
    def program(self):
        print("PROGRAM")

        # Since some newlines are required in our grammar, need to skip the excess.
        while self.checkToken(TokenType.NEWLINE):
            self.nextToken()

        # Parse all the statements in the program.
        while not self.checkToken(TokenType.EOF):
            self.statement()

        # Check that each label referenced in a GOTO is declared.
        for label in self.labelsGotoed:
            if label not in self.labelsDeclared:
                self.abort("Attempting to GOTO to undeclared label: " + label)
```

That takes care of labels. The last thing the parser needs to do is
check that variables are declared. In a LET or INPUT statement, we will
add it to a set of declared variables if it doesn\'t already exist. If a
variable is referenced in an expression, the parser will check that it
has been declared first.

Update LET in **statement**:

``` {.prettyprint .lang-python}
        # "LET" ident = expression
        elif self.checkToken(TokenType.LET):
            self.nextToken()

            #  Check if ident exists in symbol table. If not, declare it.
            if self.curToken.text not in self.symbols:
                self.symbols.add(self.curToken.text)

            self.match(TokenType.IDENT)
            self.match(TokenType.EQ)
            
            self.expression()
```

Update INPUT in **statement**:

``` {.prettyprint .lang-python}
        # "INPUT" ident
        elif self.checkToken(TokenType.INPUT):
            self.nextToken()

            #If variable doesn't already exist, declare it.
            if self.curToken.text not in self.symbols:
                self.symbols.add(self.curToken.text)

            self.match(TokenType.IDENT)
```

Then update **primary**:

``` {.prettyprint .lang-python}
    # primary ::= number | ident
    def primary(self):
        print("PRIMARY (" + self.curToken.text + ")")

        if self.checkToken(TokenType.NUMBER): 
            self.nextToken()
        elif self.checkToken(TokenType.IDENT):
            # Ensure the variable already exists.
            if self.curToken.text not in self.symbols:
                self.abort("Referencing variable before assignment: " + self.curToken.text)
            self.nextToken()
        else:
            # Error!
            self.abort("Unexpected token at " + self.curToken.text)
```

Those are the last changes to the parser! Now test it with the same
input as last time and you should see\...

``` prettyprint
Teeny Tiny Compiler
PROGRAM
STATEMENT-PRINT
EXPRESSION
TERM
UNARY
Error! Referencing variable before assignment: index
```

Now it reports an error, as it should.

Try running the compiler with one final input that is a bit more
complex:

``` prettyprint
PRINT "How many fibonacci numbers do you want?"
INPUT nums
PRINT ""

LET a = 0
LET b = 1
WHILE nums > 0 REPEAT
    PRINT a
    LET c = a + b
    LET a = b
    LET b = c
    LET nums = nums - 1
ENDWHILE
```

:) Congratulations, our Teeny Tiny parser is complete! The output may
not be pretty at the moment, but doing this has made it *very* easy for
the next phase: emitting code. The source code can be found in the
Github [repo](https://github.com/AZHenley/teenytinycompiler).

------------------------------------------------------------------------

Continue on to [part 3](teenytinycompiler3.html) of this tutorial to
learn how to emit code from the compiler. Other recommended reading:

-   Parsing ([Wikipedia](https://en.wikipedia.org/wiki/Parsing))
-   Writing an Interpreter in Go ([Amazon](https://amzn.to/2Saf28j))
-   Crafting Interpreters ([Amazon](https://amzn.to/3l8FePX),
    [web](https://craftinginterpreters.com/contents.html))

\
\
[There are Amazon affiliate links on this page.]{.small}\
\
\
\




## Austin Z. Henley {#austin-z.-henley style="margin-bottom:-2px;"}

I work on software.





<azh321@gmail.com>\
[\@austinzhenley](https://twitter.com/austinzhenley)\
[github/AZHenley](https://github.com/AZHenley)\





------------------------------------------------------------------------


[Home](../index.html) \| [Publications](../publications.html) \|
[Blog](../blog.html){style="text-decoration: underline;"}





------------------------------------------------------------------------

# Let\'s make a Teeny Tiny compiler, part 3 {#lets-make-a-teeny-tiny-compiler-part-3 .blogtitle}

[7/5/2020]{.small}\
\
![](img/20240225_knightdragon3.jpeg){.center
style="max-width:60%;border:1px solid black;"}

We are finally here. It is time to create the emitter for our Teeny Tiny
compiler, which will give us the foundation to a working compiler. The
fame and fortune is so close! Previously, we implemented the lexer
([part 1](teenytinycompiler1.html)) and the parser ([part
2](teenytinycompiler2.html)). The source code from this tutorial can be
found in the GitHub
[repo](https://github.com/AZHenley/teenytinycompiler).

The emitter is the component that will produce the compiled code. In
this case, our compiler will be producing C code. Luckily, we designed
our parser in such a way that will make emitting C code quite easy!
Since C is so ubiquitous, we will rely on your favorite C compiler
(e.g., GCC or Clang) to produce the executable for us. This means our
compiler will be platform independent without dealing with assembly code
or complex compiler frameworks.

Back before I started coding this compiler, I wrote several fictitious
examples of Teeny Tiny code and the corresponding C code that I think
the compiler should generate. This was a good exercise to see which
things translate nicely (i.e., one line of Teeny Tiny equals one line of
C) and what doesn\'t.

Let\'s take a look at an example of Teeny Tiny code and the equivalent C
code that we would like for our compiler to emit. The example is a
program that calculates *nums* values of the Fibonacci sequence.



``` prettyprint
PRINT "How many fibonacci numbers do you want?"
INPUT nums
PRINT ""

LET a = 0
LET b = 1
WHILE nums > 0 REPEAT
    PRINT a
    LET c = a + b
    LET a = b
    LET b = c
    LET nums = nums - 1
ENDWHILE
```



``` {.prettyprint .lang-c}
#include <stdio.h>

int main(void){
    float nums, a, b, c;

    printf("How many fibonacci numbers do you want?\n");
    scanf("%f", &nums);
    printf("\n");

    a = 0;
    b = 1;
    while(nums>0){
        printf("%.2f\n", (float)(a));
        c = a+b;
        a = b;
        b = c;
        nums = nums-1;
    }
    
    return 0;
}
```



The above code *almost* translates line by line. Like *PRINT* turns into
*printf*, *INPUT* to *scanf*, etc. You\'ll also notice there are other
differences. The C code has a main function, includes a library, and
returns 0 at the end. You\'ll also see that the C code has different
syntax for variable declarations versus variable assignments (unlike our
LET statement syntax which combines declarations and assignments) and
that they are all declared at the top (this is an old C convention). It
is these differences that our compiler needs to know about so that they
can be handled accordingly. There are a few more that will come up
later, but never fear, we can address them!

### Emitter Overview

So how does this emitter work??? In each function of the parser, we will
call the emitter to produce the appropriate C code. The emitter is
effectively just appending a bunch of strings together while following
along the parse tree. For each grammar rule of Teeny Tiny, we will
figure out how it should map to C code.

Below is an illustration of compiling a single line of Teeny Tiny code.
It shows each step of how the lexer, parser, and emitter work together.
The left shows the input code with the relevant token highlighted, the
center shows the parse tree with the current element highlighted, and
the right side shows the emitted code with the newly appended text
highlighted.

![](img/20240225_teenyemitter.gif){.center style="max-width:90%"}

Try watching the animation a few times. The emitter is utilizing the
parse tree to emit the C code in fragments. This will all make more
sense when we get to the code.

### Code Setup

First, we will update **main** in **teenytiny.py** to use the emitter
that we will soon build:

``` {.prettyprint .lang-python}
from lex import *
from emit import *
from parse import *
import sys

def main():
    print("Teeny Tiny Compiler")

    if len(sys.argv) != 2:
        sys.exit("Error: Compiler needs source file as argument.")
    with open(sys.argv[1], 'r') as inputFile:
        source = inputFile.read()

    # Initialize the lexer, emitter, and parser.
    lexer = Lexer(source)
    emitter = Emitter("out.c")
    parser = Parser(lexer, emitter)

    parser.program() # Start the parser.
    emitter.writeFile() # Write the output to file.
    print("Compiling completed.")

main()
    
```

The compiler still expects a filename as a command line argument to open
for input. But now the parser object will control the lexer and the
emitter. So next, we have to go on and implement the emitter. Create
**emit.py** with the following code:

``` {.prettyprint .lang-python}
# Emitter object keeps track of the generated code and outputs it.
class Emitter:
    def __init__(self, fullPath):
        self.fullPath = fullPath
        self.header = ""
        self.code = ""

    def emit(self, code):
        self.code += code

    def emitLine(self, code):
        self.code += code + '\n'

    def headerLine(self, code):
        self.header += code + '\n'

    def writeFile(self):
        with open(self.fullPath, 'w') as outputFile:
            outputFile.write(self.header + self.code)
```

That is the entirety of the emitter\'s code! It is simply a helper class
for appending strings together. *code* is the string containing the C
code that is emitted, *header* contains things that we will prepend to
the code later on, and *fullPath* is the path to write the file
containing C code. We use **emit** to add a fragment of C code and
**emitLine** to add a fragment that ends a line. **headerLine** is for
adding a line of C code to the top of the C code file, such as including
a library header, the main function, and variable declarations. Lastly,
**writeFile** writes the C code to a file.

We then have to make a small change to the parser\'s **\_\_init\_\_** in
**parse.py** to use the emitter:

``` {.prettyprint .lang-python}
    def __init__(self, lexer, emitter):
        self.lexer = lexer
        self.emitter = emitter

        self.symbols = set()    # All variables we have declared so far.
        self.labelsDeclared = set() # Keep track of all labels declared
        self.labelsGotoed = set() # All labels goto'ed, so we know if they exist or not.

        self.curToken = None
        self.peekToken = None
        self.nextToken()
        self.nextToken()    # Call this twice to initialize current and peek.
```

That really is all the emitter entails. Now we just have to call it with
the appropriate C code from the parser.

### Emitting Statements

We will now be modifying the existing functions in **parse.py**. We will
be calling the emitter functions from inside the parser (and removing
the print statements that we used for testing in part 2).

Going through the functions in order, let\'s look at **program**. To
translate an empty Teeny Tiny program, we need some boilerplate C code.
Really, we only need a main function but we will go ahead and include
*stdio.h* so that *printf* and *scanf* are available. Update **program**
with:

``` {.prettyprint .lang-python}
    # program ::= {statement}
    def program(self):
        self.emitter.headerLine("#include <stdio.h>")
        self.emitter.headerLine("int main(void){")
        
        # Since some newlines are required in our grammar, need to skip the excess.
        while self.checkToken(TokenType.NEWLINE):
            self.nextToken()

        # Parse all the statements in the program.
        while not self.checkToken(TokenType.EOF):
            self.statement()

        # Wrap things up.
        self.emitter.emitLine("return 0;")
        self.emitter.emitLine("}")

        # Check that each label referenced in a GOTO is declared.
        for label in self.labelsGotoed:
            if label not in self.labelsDeclared:
                self.abort("Attempting to GOTO to undeclared label: " + label)
```

The additions here are starting out with the include statement and the
main function. Then we loop through all of the *self.statement()*. At
the end, we have to close up the main function with *return 0;* and a
closing curly bracket.

We will continue to use the emitter just like so. Emit some initial
code, call other parser functions based on the grammar, and emit more
code before returning up the call stack.

Let\'s test our emitter thus far. Assuming your **hello.teeny** with the
Fibonacci code example still exists from last time, run *python3
teenytiny.py hello.teeny*. If things go as planned, the last line in the
terminal should be \"Compiling completed.\" and **out.c** should have
been created. Check the contents of the emitted code and you should see:

``` {.prettyprint .lang-c}
#include <stdio.h>
int main(void){
return 0;
}
```

OUR COMPILER JUST COMPILED SOMETHING!

That is right, our Teeny Tiny compiler can now emit a working C program.
(Ignore the lack of code formatting\...) If you are on Mac or Linux then
you can run *clang out.c* or *gcc out.c* to create an executable, and
then *./a.out* to run the program. It won\'t output anything just yet.
Soon though. Very soon. If you don\'t have access to a C compiler, you
can use [Repl.it](https://repl.it/languages/c) to run C code in the
browser.

Next we will update *PRINT* inside the **statement** function of
**parse.py**. Recall that there are two cases for the print statement:
printing a string and printing the result of an expression. Here is the
code:

``` {.prettyprint .lang-python}
        # "PRINT" (expression | string)
        if self.checkToken(TokenType.PRINT):
            self.nextToken()

            if self.checkToken(TokenType.STRING):
                # Simple string, so print it.
                self.emitter.emitLine("printf(\"" + self.curToken.text + "\\n\");")
                self.nextToken()

            else:
                # Expect an expression and print the result as a float.
                self.emitter.emit("printf(\"%" + ".2f\\n\", (float)(")
                self.expression()
                self.emitter.emitLine("));")
```

Look at how we are emitting fragments of C code. Hopefully this is
starting to make more sense.

Run your compiler with **hello.teeny** again and check **out.c**:

``` {.prettyprint .lang-c}
#include <stdio.h>
int main(void){
printf("How many fibonacci numbers do you want?\n");
printf("\n");
printf("%.2f\n", (float)());
return 0;
}
```

This is fantastic. Teeny Tiny now emits code for printing strings. Run
this with your favorite C compiler and it will work. One thing to note
is that the third *printf* isn\'t doing anything. This is because
**expression** isn\'t emitting anything yet. Eventually this will
produce a number.

Continuing through the parser, let\'s replace the code for *IF* and
*WHILE* statements. They look almost identical:

``` {.prettyprint .lang-python}
        # "IF" comparison "THEN" block "ENDIF"
        elif self.checkToken(TokenType.IF):
            self.nextToken()
            self.emitter.emit("if(")
            self.comparison()

            self.match(TokenType.THEN)
            self.nl()
            self.emitter.emitLine("){")

            # Zero or more statements in the body.
            while not self.checkToken(TokenType.ENDIF):
                self.statement()

            self.match(TokenType.ENDIF)
            self.emitter.emitLine("}")

        # "WHILE" comparison "REPEAT" block "ENDWHILE"
        elif self.checkToken(TokenType.WHILE):
            self.nextToken()
            self.emitter.emit("while(")
            self.comparison()

            self.match(TokenType.REPEAT)
            self.nl()
            self.emitter.emitLine("){")

            # Zero or more statements in the loop body.
            while not self.checkToken(TokenType.ENDWHILE):
                self.statement()

            self.match(TokenType.ENDWHILE)
            self.emitter.emitLine("}")
```

If you test this code with the Fibonacci example, you\'ll find something
very broken. The C code contains *while(){*. Uhoh. This is the same
problem as before, nothing is being emitted by functions like
**expression** and **comparison**. But the rest of the loop looks
correct.

The *LABEL* and *GOTO* code is fairly straightforward:

``` {.prettyprint .lang-python}
        # "LABEL" ident
        elif self.checkToken(TokenType.LABEL):
            self.nextToken()

            # Make sure this label doesn't already exist.
            if self.curToken.text in self.labelsDeclared:
                self.abort("Label already exists: " + self.curToken.text)
            self.labelsDeclared.add(self.curToken.text)

            self.emitter.emitLine(self.curToken.text + ":")
            self.match(TokenType.IDENT)

        # "GOTO" ident
        elif self.checkToken(TokenType.GOTO):
            self.nextToken()
            self.labelsGotoed.add(self.curToken.text)
            self.emitter.emitLine("goto " + self.curToken.text + ";")
            self.match(TokenType.IDENT)
```

We will skip testing everything for a moment to get to something more
interesting.

The *LET* code is a bit different. It will sometimes call
**emitter.headerLine**. Why? Because the first time a variable is
referenced in Teeny Tiny it should emit a variable declaration in C, and
place it at the top of the main function (this is an old C convention).
As I alluded to earlier, this is one of the major differences in syntax
between Teeny Tiny and C, so our compiler has to do a little more work
to translate. Teeny Tiny doesn\'t differentiate between variable
declarations and assignments, but C does. Here is the code:

``` {.prettyprint .lang-python}
        # "LET" ident = expression
        elif self.checkToken(TokenType.LET):
            self.nextToken()

            #  Check if ident exists in symbol table. If not, declare it.
            if self.curToken.text not in self.symbols:
                self.symbols.add(self.curToken.text)
                self.emitter.headerLine("float " + self.curToken.text + ";")

            self.emitter.emit(self.curToken.text + " = ")
            self.match(TokenType.IDENT)
            self.match(TokenType.EQ)
            
            self.expression()
            self.emitter.emitLine(";")
```

Ok ok, compile **hello.teeny** and inspect **out.c**\...

``` {.prettyprint .lang-c}
#include <stdio.h>
int main(void){
float a;
float b;
float c;
printf("How many fibonacci numbers do you want?\n");
printf("\n");
a = ;
b = ;
while(){
printf("%.2f\n", (float)());
c = ;
a = ;
b = ;
nums = ;
}
return 0;
}
```

The code definitely isn\'t error free, plus things are a bit hard to
read without formatting. You\'ll see that the variable declarations are
all at the top though! That is exciting. So what isn\'t working? Well *a
= ;* certainly isn\'t valid C code. Looks like all the variable
assignments are broken. In fact, anything that calls **expression** is
emitting invalid code (still). We are making significant progress
though!

Alright, we have one more type of statement to support: *INPUT*. There
are a few things to note about this. First, if the variable being
referenced doesn\'t already exist, then we should declare it by using
**emitter.headerLine**. Second, we have to include some C specific code
because of how *scanf* works. We could just emit *scanf(\"%f\", &foo);*,
but that won\'t handle invalid input, such as when a user enters a
letter. So we must also check if scanf returns 0. If it does, we clear
the input buffer and we set the input variable to 0.

Note: This will be a limitation of Teeny Tiny. You can\'t tell if the
user input was the value 0 or an invalid input. They are treated the
same. There are ways to fix this though. You could modify it such that
an invalid input results in an obscure value, like -999, or prints an
error message and asks for a valid input in a loop, or sets an error
code in a flag. Every programming language handles these types of
scenarios differently. The value of 0 will work for now though.

Let\'s venture forth with the code. It looks a bit ugly, but it is
really just some boilerplate for handling invalid input. Here it is:

``` {.prettyprint .lang-python}
        # "INPUT" ident
        elif self.checkToken(TokenType.INPUT):
            self.nextToken()

            # If variable doesn't already exist, declare it.
            if self.curToken.text not in self.symbols:
                self.symbols.add(self.curToken.text)
                self.emitter.headerLine("float " + self.curToken.text + ";")

            # Emit scanf but also validate the input. If invalid, set the variable to 0 and clear the input.
            self.emitter.emitLine("if(0 == scanf(\"%" + "f\", &" + self.curToken.text + ")) {")
            self.emitter.emitLine(self.curToken.text + " = 0;")
            self.emitter.emit("scanf(\"%")
            self.emitter.emitLine("*s\");")
            self.emitter.emitLine("}")
            self.match(TokenType.IDENT)
```

Go ahead and test this. All of our statements now emit C code!

### Emitting Expressions

The last remaining thing to emit code for are expressions. This is
actually very little work since Teeny Tiny code requires essentially no
changes in order to be valid C code, so just emit it as is. What I mean
is that *index \* offset + 1* in Teeny Tiny is also *index \* offset +
1* in C.

We will first take care of **comparison**, which will only emit the
specific operator.

``` {.prettyprint .lang-python}
    # comparison ::= expression (("==" | "!=" | ">" | ">=" | "<" | "<=") expression)+
    def comparison(self):
        self.expression()
        # Must be at least one comparison operator and another expression.
        if self.isComparisonOperator():
            self.emitter.emit(self.curToken.text)
            self.nextToken()
            self.expression()
        # Can have 0 or more comparison operator and expressions.
        while self.isComparisonOperator():
            self.emitter.emit(self.curToken.text)
            self.nextToken()
            self.expression()
```

In fact, this is so simple that **expression**, **term**, and **unary**
also work the same way: just emit the operator.

``` {.prettyprint .lang-python}
    # expression ::= term {( "-" | "+" ) term}
    def expression(self):
        self.term()
        # Can have 0 or more +/- and expressions.
        while self.checkToken(TokenType.PLUS) or self.checkToken(TokenType.MINUS):
            self.emitter.emit(self.curToken.text)
            self.nextToken()
            self.term()


    # term ::= unary {( "/" | "*" ) unary}
    def term(self):
        self.unary()
        # Can have 0 or more *// and expressions.
        while self.checkToken(TokenType.ASTERISK) or self.checkToken(TokenType.SLASH):
            self.emitter.emit(self.curToken.text)
            self.nextToken()
            self.unary()


    # unary ::= ["+" | "-"] primary
    def unary(self):
        # Optional unary +/-
        if self.checkToken(TokenType.PLUS) or self.checkToken(TokenType.MINUS):
            self.emitter.emit(self.curToken.text)
            self.nextToken()        
        self.primary()
```

We have emitted code from all of our grammar functions except
**primary**. Once we reach this point, the only thing to emit is either
the number literal or the variable identifier, like so:

``` {.prettyprint .lang-python}
    # primary ::= number | ident
    def primary(self):
        if self.checkToken(TokenType.NUMBER): 
            self.emitter.emit(self.curToken.text)
            self.nextToken()
        elif self.checkToken(TokenType.IDENT):
            # Ensure the variable already exists.
            if self.curToken.text not in self.symbols:
                self.abort("Referencing variable before assignment: " + self.curToken.text)

            self.emitter.emit(self.curToken.text)
            self.nextToken()
        else:
            # Error!
            self.abort("Unexpected token at " + self.curToken.text)
```

While we are here, go ahead and remove the *print* from **nl**.

That is it! Teeny Tiny compiles to C code now! Let\'s test it with a new
example, **average.teeny**:

``` prettyprint
# Compute average of given values.

LET a = 0
WHILE a < 1 REPEAT
    PRINT "Enter number of scores: "
    INPUT a
ENDWHILE

LET b = 0
LET s = 0
PRINT "Enter one value at a time: "
WHILE b < a REPEAT
    INPUT c
    LET s = s + c
    LET b = b + 1
ENDWHILE

PRINT "Average: "
PRINT s / a
```

When you run Teeny Tiny on this, all you should see is:

``` prettyprint
Teeny Tiny Compiler
Compiling completed.
```

If you take a look at **out.c**, you\'ll see this glorious code:

``` {.prettyprint .lang-c}
#include <stdio.h>
int main(void){
float a;
float b;
float s;
float c;
a = 0;
while(a<1){
printf("Enter number of scores: \n");
if(0 == scanf("%f", &a)) {
a = 0;
scanf("%*s");
}
}
b = 0;
s = 0;
printf("Enter one value at a time: \n");
while(b<a){
if(0 == scanf("%f", &c)) {
c = 0;
scanf("%*s");
}
s = s+c;
b = b+1;
}
printf("Average: \n");
printf("%.2f\n", (float)(s/a));
return 0;
}        
```

Run it through a C compiler. Wow. Look at that. We made a real compiler.
Move over Borland!

### Compiling Teeny Tiny

You may notice that it is bit tedious to use Teeny Tiny. First, we have
to run the Python script. Second, we run the C compiler. Third, we
execute the program. There really shouldn\'t be so many steps. You could
build this all into Teeny Tiny or you could write a script to automate
it.

My friend, [Stephen Marz](https://blog.stephenmarz.com/), made a Bash
script that I find helpful:

``` {.prettyprint .lang-bash}
PYTHON="python3"
COMPILER="teenytiny.py"
CC="gcc"

function comp {
    BN=$(basename -s .teeny $1)
    TTOUTPUT=$(${PYTHON} ${COMPILER} $1 2>&1)
    if [ $? -ne 0 ]; then
        echo "${TTOUTPUT}"
    else
        mv out.c ${BN}.c
        CCOUTPUT=$(${CC} -o ${BN} ${BN}.c)
        if [ $? -ne 0 ]; then
            echo "${CCOUTPUT}"
        else
            echo "${TTOUTPUT}"
        fi
    fi
}

if [ $# -eq 0 ]; then
    for i in $(ls examples/*.teeny); do
        comp $i
    done
else
    comp $1
fi
```

You can run it by doing *bash build.sh hello.teeny* in the terminal, and
then you run the executable like *./hello* .

![](img/20240225_teenytinyoutput3.png){.center style="max-width:80%;"}

### The project continues\...

Your Teeny Tiny compiler is working! Given code written in our own
language, the compiler will produce working C code that can be compiled
and executed. This is quite the feat. If you add a few more features,
you could make a Teeny Tiny compiler that is written in Teeny Tiny code!
Compilers all the way down.

The complete source code for Teeny Tiny can be found in the Github
[repo](https://github.com/AZHenley/teenytinycompiler).

Although the tutorial ends here, your compiler adventure doesn\'t have
to. You could switch up the syntax or there are numerous features that
you can add to incrementally improve the language and compiler. For
example:

-   Parentheses for expressions
-   Logical operators (and, or, not)
-   ELSE IF and ELSE
-   FOR loop
-   Number literals written in binary, hex, and octal
-   Better compiler errors (e.g., what line the error occurred)
-   Allow multiple code files
-   Functions with parameters and return values
-   Lexical scope (see
    [scope](https://en.wikipedia.org/wiki/Scope_(computer_science)))
-   Standard library (e.g., file operations)
-   [Abstract syntax
    tree](https://en.wikipedia.org/wiki/Abstract_syntax_tree)
    representation
-   More primitive types (e.g., integer, strings, boolean)
-   Arrays
-   Record types (i.e., structs or tuples)
-   Type checking (see [type
    systems](https://en.wikipedia.org/wiki/Type_system))
-   Compiler optimizations (e.g., [constant
    folding](https://en.wikipedia.org/wiki/Constant_folding))
-   Test cases for the compiler (see [unit
    testing](https://en.wikipedia.org/wiki/Unit_testing) and
    [test-driven
    development](https://en.wikipedia.org/wiki/Test-driven_development))

Most notable is the abstract syntax tree (AST) representation. This is
one of the major things that this tutorial left out that basically all
compilers use. It is an intermediate representation of the code that
enables you to perform all sorts of analyses and optimizations on the
code before the emitting stage. Fortunately, the way we built our
compiler means that it is extremely easy to produce an AST!

------------------------------------------------------------------------

Reach out to me and let me know what I should write about next because I
doubt I\'m done with the Teeny Tiny project. Until then, I also highly
recommend reading these longer tutorials:

-   Writing an Interpreter in Go ([Amazon](https://amzn.to/2Saf28j))
-   Crafting Interpreters ([Amazon](https://amzn.to/3l8FePX),
    [web](https://craftinginterpreters.com/contents.html))

\
\
[There are Amazon affiliate links on this page.]{.small}\
\
\
\
